3단계: 가지 단계 – 모델 구조 만들기
==================================

이 단계에서는 CNN 기반 모델을 설계하고 구조를 이해합니다.

3.1 CNN의 핵심
--------------

**합성곱 신경망(CNN)이 이미지에 특화된 이유**
- **지역적 연결**: 인접한 픽셀들만 연결하여 이미지의 공간적 구조 보존
- **가중치 공유**: 같은 필터를 전체 이미지에 적용하여 패턴 감지
- **변환 불변성**: 객체의 위치가 바뀌어도 동일하게 인식
- **계층적 특징 추출**: 낮은 층에서 엣지, 높은 층에서 복합 패턴 학습

**CNN의 주요 개념**
- **필터(Filter/Kernel)**: 특징을 추출하는 작은 행렬
- **특성 맵(Feature Map)**: 필터 적용 결과
- **수용 영역(Receptive Field)**: 하나의 뉴런이 보는 입력 영역

3.2 레이어 역할 및 순서
----------------------

**입력층 (Input Layer)**
- 역할: 원본 이미지 데이터 받기
- 형태: (높이, 너비, 채널)
- 예시: (224, 224, 3) - 224×224 RGB 이미지

**합성곱층 (Convolutional Layer)**
- 역할: 이미지에서 특징 추출
- 파라미터:
  - **필터 개수**: 추출할 특징의 종류
  - **필터 크기**: 보통 3×3, 5×5
  - **스트라이드**: 필터 이동 간격
  - **패딩**: 가장자리 처리 방법
- 효과: 엣지, 모서리, 텍스처 등 감지

**활성화 함수 (Activation Function)**
- 역할: 비선형성 추가로 복잡한 패턴 학습 가능
- 종류:
  - **ReLU**: 0 이하는 0, 양수는 그대로 (가장 일반적)
  - **Sigmoid**: 0~1 범위로 압축
  - **Tanh**: -1~1 범위로 압축
- 배치: 각 합성곱층 뒤에 적용

**풀링층 (Pooling Layer)**
- 역할: 특성 맵 크기 축소, 중요한 정보만 보존
- 종류:
  - **Max Pooling**: 영역 내 최댓값 선택
  - **Average Pooling**: 영역 내 평균값 계산
- 효과: 계산량 감소, 과적합 방지, 위치 변화에 강건

**정규화층 (Normalization Layer)**
- 역할: 학습 안정성 향상
- 종류:
  - **Batch Normalization**: 배치 단위로 정규화
  - **Layer Normalization**: 레이어 단위로 정규화
- 효과: 빠른 학습, 그래디언트 폭발/소실 방지

**드롭아웃층 (Dropout Layer)**
- 역할: 과적합 방지
- 방법: 학습 시 일부 뉴런을 무작위로 비활성화
- 비율: 보통 0.2~0.5 (20%~50% 비활성화)

**완전연결층 (Fully Connected Layer)**
- 역할: 추출된 특징으로 최종 분류/예측
- 위치: 모델의 마지막 부분
- 구조: 모든 뉴런이 연결된 일반적인 신경망층

**출력층 (Output Layer)**
- 역할: 최종 예측 결과 출력
- 분류: 클래스 개수만큼 뉴런 (Softmax 활성화)
- 회귀: 1개 뉴런 (선형 활성화)

3.3 모델 설계 실습 블록
----------------------

**기본 CNN 구조 예시**
```
입력 이미지 (224×224×3)
    ↓
Conv2D (32필터, 3×3) + ReLU
    ↓
MaxPool2D (2×2)
    ↓
Conv2D (64필터, 3×3) + ReLU
    ↓
MaxPool2D (2×2)
    ↓
Conv2D (128필터, 3×3) + ReLU
    ↓
Flatten (1차원으로 변환)
    ↓
Dense (128뉴런) + ReLU + Dropout(0.5)
    ↓
Dense (클래스 수) + Softmax
    ↓
출력 (클래스 확률)
```

**설계 고려사항**

**1) 모델 깊이**
- 얕은 모델: 간단한 특징, 빠른 학습, 제한된 표현력
- 깊은 모델: 복잡한 특징, 느린 학습, 높은 표현력

**2) 필터 수 증가 패턴**
- 일반적 패턴: 32 → 64 → 128 → 256
- 이유: 깊어질수록 더 복잡한 특징 추출

**3) 풀링 전략**
- 위치: 보통 합성곱층 2-3개마다 적용
- 효과: 특성 맵 크기 절반으로 축소

**4) 정규화 배치**
- Batch Norm: Conv2D → BatchNorm → ReLU 순서
- Dropout: 완전연결층에서 주로 사용

모델 설계 체크리스트
-------------------
- [ ] 입력 이미지 크기 결정
- [ ] 합성곱층 개수 및 필터 수 설정
- [ ] 풀링층 배치 결정
- [ ] 활성화 함수 선택
- [ ] 정규화 기법 적용
- [ ] 드롭아웃 비율 설정
- [ ] 출력층 구조 설계

다음 단계 준비
--------------
모델 구조 설계가 완료되면 열매 단계(학습 및 실험)로 진행하여 실제로 모델을 훈련시키고 성능을 최적화합니다.